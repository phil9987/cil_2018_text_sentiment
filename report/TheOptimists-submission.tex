\documentclass[10pt,conference,compsocconf]{IEEEtran}

%\usepackage{times}
%\usepackage{balance}
\usepackage{url}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}

\newcommand{\todo}[1]{}
\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}

\newcommand{\pisch}[1]{\textit{\textcolor{blue}{pisch: #1}}}

\begin{document}
\title{CIL 2018: Text Sentiment Classification}

\author{
  Aryaman Fasciati, Nikolas G\"obel, Philip Junker, Pirmin Schmid\\
  The Optimists\\
  Department of Computer Science, ETH Zurich, Switzerland
}

\maketitle

\begin{abstract}
  In this work we consider the task of classifying tweets by
  sentiment. In particular we want to determine whether a given tweet,
  which has been stripped of emoticons, used to include a positive
  smiley ``:)'' or a negative smiley ``:(''. We present three models,
  a random forest classifier, a simple stacked GRU/RNN model, and a
  more complex neural network combining GRU/RNN with convolutional
  layers. We train these models on GloVe word embeddings, pre-trained
  on 2.5 billion tweets. Our final model achieves a classification accuracy
  of 86.6\%.
\end{abstract}

\section{Introduction}

The global prevalence of social media has enabled a massive amount of users
to publicly voice their opinions and interact with each other.
Microblogging services like Twitter\footnote{www.twitter.com} make it
very easy for users to comment on current events, tell other users about their
lives, and discuss the products and services they use.
This data can be analysed for sentiments and opinions and can be used for
various purposes, such as brand monitoring or stock market prediction.

The goal of this project is to build a sentiment classifier that
predicts whether a Twitter tweet used to include a positive smiley :) or
a negative smiley :(, based on the remaining text.

Our first baseline uses random forests and achieved an accuracy of
72\%. Our second baseline uses a Recurrent Neural Network (RNN) model
with an accuracy of \todo{rnn-baseline-accuracy}.

In a third model, we refined our second baseline, incorporating
\todo{describe novel approach}, in a RNN-based approach. This model
achieved 86.6\% accuracy.

\section{Related Work}

Recurrent neural networks with their applicability to
sequence-to-sequence tasks have been used across many tasks,
especially in the NLP domain, such as generating text \cite{Sutskever}.
Although RNNs seem to be primarily used for problems more
complex than binary classification, we were interested in exploring
the application of short-term memory to our task. This can be achieved
via the Long short-term memory (LSTM) or Gated recurrent unit (GRU)
variants of RNNs. Both have found extensive use in practical
applications, such as predictive chat replies \cite{allo},
question answering \cite{DiWang}, and video to text \cite{DBLP}.

\section{Models}

\subsection{First Baseline (B1)} \label{b1}

Our first baseline uses a random forest model with unlimited
max\_depth and 20 estimators. We used the random forest implementation
provided with the scikit-learn library \cite{scikit-learn}.

Tweets from the datasets are run through a pre-processing script,
provided by the GloVe project. The pre-processor normalizes tweets,
for example by replacing user names with a single token indicating a
mention. More importantly, symbols used commonly in tweets, such as
the heart ``<3'', are converted into tokens that are contained in the
vocabulary.

Input tweets are then split into words. Each word is looked up in the
embedding dictionary. We use 200 dimensional word vectors. Words that
are not in the vocabulary are ignored. All word embeddings for a given
tweet are then aggregated into a single vector, by computing their
mean. Word embeddings are provided by the GloVe \cite{glove} project
from Stanford, pre-trained on two billion tweets.

The classifier is then trained on these tweet embeddings. We expected
this relatively simple approach to already perform quite well, because
a small number of features of single words would seem to have an
impact on overall tweet sentiment in practice. Still, aggregating word
embeddings destroys a lot of information contained in the order of
words. In particular, this approach does not differentiate between
permutations of similar words at all. It also assigns equal weight to
every word when computing the mean, whereas we would commonly expect a
small subset of words to be disproportionally strong indicators of
sentiment (``love'', ``hate'', ``good'', ``bad'', etc...).

This model achieved an accuracy of 72\%.

\subsection{Second Baseline (B2)} \label{b2}

For the second baseline, we trained a stacked, recurrent neural
network. Two GRU cells were stacked; a hidden state size of 384 was used.
In contrast to the random forest baseline, tweet embeddings
were not aggregated in any way from their words. Rather, the network
was trained on matrices of dimension \(word\_count_{tweet} \times 200\).
Words not known in the embedding vocabulary were ignored.
Statistical analysis of the provided data in \autoref{fig:wordcount} shows that
the majority ($>$98\%) of tweets have less than 30 words.
Tweets longer than that were ignored during training and shortened for prediction,
whereas shorter tweets were padded.

All output of the RNN was used as input of a single layer fully connected NN
to 2 nodes with a sigmoid activation function.
Argmax was used to determine the sentiment.
Loss was calculated with sparse softmax cross entropy with logits and
an Adam optimizer was used for learning with clipped gradient at 10
and a learning rate of $10^{-4}$.
In total, the model was trained for 4 epochs on Leonhard.
98\% of the training data was used for training; 2\% was used for evaluation.

Smaller and larger hidden state sizes were tested but resulted in
lower accuracy. Additonally, we experimented with LSTM cells, which did not give us
a higher score than with GRU cells.
All outputs of the RNN were used as input of the NN to bring more state
information into the final classifier step. Using only the final hidde
state has been tested and gave lower accuracy.
And finally, a small NN was used to handle the binary classification from the RNN 
state to allow also for non-linear transformation in contrast to the classic 
transformation with a matrix multiplication and potentially adding of a
bias offset.

\pisch{not yet sure about this discussion points here}
In contrast to B1, this model does take word order into account
and allows for different weights to be assigned to certain,
sentiment-implying words, during training. Intuitively, we would
expect the latter to result in significantly higher classification
accuracy, compared to the random forest model. It is not as clear,
whether taking word order into account during training is important
for the task of sentiment classification and would thus dilute inputs
in a sense.

This model achieved an accuracy of 85\%.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.53]{word_count_histogram.png}
  \caption{Word Count Distribution}
  \label{fig:wordcount}
\end{figure}


\subsection{RNN + Convolution} \label{our-model}

For our final model, we decided to expand on the promising RNN
approach taken in baseline B2. We can consider the RNN output to
represent learned tweet features, which bear significance for
sentiment classification. Consequently, we wanted to re-combine these
features in a structure-preserving, context-sensitive
manner. Convolutional neural networks fit this bill and in addition
possess the property of being invariant w.r.t
translation. Intuitively, this sounded useful to the task at hand,
because we expect certain word sequences to indicate sentiment
independent of where they appear inside a tweet.

We therefore introduced two one-dimensional convolution layers each
followed by a one-dimensional max-pooling layer. We chose convolution
window sizes of six and four respectively, with no bias and ReLu
activation. The max-pooling layers aggregate each pooling window into
a single scalar. We chose a window size of two and strides of two for
both layers. Outputs for every window are concatenated and passed on
to a fully connected rectifier. In order to lessen the impact of
overfitting, we apply a dropout rate of 0.4 during training.

The final outputs are then fed into a two-unit layer, each unit
indicating one of the two possible sentiments. Overall classification
output is then obtained by applying the \texttt{argmax} function.
The architecture of our model is summarized in \autoref{fig:ourmodel}.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.35]{our_model_architecture.png}
  \caption{Our Model}
  \label{fig:ourmodel}
\end{figure}

We trained the model for four epochs, optiziming sparse softmax cross
entropy using the Adam gradient descent algorithm. During training,
gradients are clipped to 10 to avoid instabilities (\texttt{Exploding
  Gradients Problem}). Training on the Leonhard cluster took around
three hours. The model achieved an accuracy of 86.6\%.

\section*{Other Approaches}

The model described in \autoref{our-model} significantly outperforms
the random forest classifier described in \autoref{b1} and improves
slightly upon the plain RNN approach in \autoref{b2}. In addition to
these, we experimented with various other approaches.

\subsection*{Effect of word order}

In particular, we were interested in exploring the tradeoff between
increasing the amount of information encoded in each input (e.g. bag
of words vs embedding matrix) and the additional noise invited by
doing so. For example, even though it would seem that a sophisticated
model like the RNN used in \autoref{b2} and \autoref{our-model} should
benefit from more information (word order, context) this is only true
in practice assuming sufficiently large datasets and computational
resources.

It was our suspicion that weighing words independently had a much
stronger effect on classification performance than respecting
different word orders. Therefore we re-trained the RNN model presented
in \autoref{b2} again, but modified the pre-processing steps. Instead
of providing each tweet's embedding matrix as is, we sorted the
columns by vector norm. The idea was to normalize word order, while
still preserving the ability to assign individual weights to
words.

This model achieved an accuracy of 85.2\%, confirming our hypothesis
that word order was not a significant contributor to accuracy, but
disproving the theory that respecting word order would add too much
noise for the RNN to handle.

\subsection*{Detecting negation}

We also tried to identify words within the scope of a negation using
NLTK. Embeddings of negated words would then be inverted before
feeding them into the RNN. Although detecting negation scopes and even
simple cases of double negation seemed to work very robustly, the
resulting model merely matched the accuracy of the one presented in
\autoref{our-model}, 86.7\%. On the other hand, this result underlines
the power of the LSTM/GRU cells iniside the RNN, which seem to be able
to keep track of negation scopes automatically.

\section*{Conclusions}

Overall our additions do improve on the simple RNN baseline
established in \autoref{b2}, but not by much. Both RNN models do
however improve significantly on the random forest classifier. This
could hint at the strength and relative ease of an approach based on
taking short-term memory into account. Further investigation is
neccessary to determine, whether the improvements are due to the GRU
units or simply a result of the more powerful input model (sequence of
word embeddings instead of a simple bag of words).

A drawback of the more complex model (with respect to the relatively
small increase in accuracy) is the increase in parameters (stack size,
convolution and pooling window sizes, stride lengths, dropout rate,
activation functions, etc...) that would have to be empirically
determined (via a grid search, for example). In combination with the
computational resources required to train the model, this makes it
very hard to further improve the model in a structured way.

\section*{Acknowledgements}

The authors wish to express their gratitude to the Euler and Leonhard
clusters at ETH, without whose unwavering computational effort this
project could not have been done.

\bibliographystyle{IEEEtran}
\bibliography{TheOptimists-literature}
\end{document}
